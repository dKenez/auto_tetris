{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Tetris\n",
    "## Detecting roof surfaces from satellite images with UNET architecture\n",
    "\n",
    "Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Dec  4 20:04:17 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:15:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    55W / 300W |  31935MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2...  On   | 00000000:16:00.0 Off |                    0 |\n",
      "| N/A   54C    P0   296W / 300W |  13371MiB / 32768MiB |     99%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2...  On   | 00000000:3A:00.0 Off |                    0 |\n",
      "| N/A   51C    P0   256W / 300W |  19689MiB / 32768MiB |    100%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2...  On   | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   41C    P0    58W / 300W |   1201MiB / 32768MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      4473      C   ...40/Thesis/venv/bin/python    31358MiB |\n",
      "|    0   N/A  N/A     27901      C   ...40/Thesis/venv/bin/python      574MiB |\n",
      "|    1   N/A  N/A      9043      C   ..._tetris/tetris/bin/python     3624MiB |\n",
      "|    1   N/A  N/A     23921      C   python3                          9744MiB |\n",
      "|    2   N/A  N/A     13513      C   python                           1196MiB |\n",
      "|    2   N/A  N/A     20764      C   python3                         18490MiB |\n",
      "|    3   N/A  N/A      7920      C   python                           1196MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Check for Nvidia gpus\n",
    "! nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_augmentation import load_data, create_new_data_dirs, augment_data\n",
    "from utils import del_new_data\n",
    "# https://dev.to/rohitfarmer/how-to-run-jupyter-notebook-in-an-interactive-node-on-a-high-performance-computer-hpc-27mg\n",
    "from pathlib import Path\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = Path.cwd().parent\n",
    "base_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_x, train_y), (val_x, val_y), (test_x, test_y) = load_data(\n",
    "    base_path,\n",
    "    split=(70, 15, 15),\n",
    "    shuffle=True,\n",
    "    # max_items=200  # how many images to create dataset from, default is all images\n",
    ")\n",
    "print(f\"{len(train_x)=} - {len(train_y)=}\")\n",
    "print(f\"{len(val_x)=} - {len(val_y)=}\")\n",
    "print(f\"{len(test_x)=} - {len(test_y)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders for the new data in project root\n",
    "del_new_data()\n",
    "create_new_data_dirs(base_path)\n",
    "\n",
    "# data augmentation\n",
    "augment_data(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    base_path / \"data\",\n",
    "    base_path / \"new_data/train\",\n",
    "    augment=False,\n",
    ")\n",
    "augment_data(\n",
    "    val_x,\n",
    "    val_y,\n",
    "    base_path / \"data\",\n",
    "    base_path / \"new_data/val\",\n",
    "    augment=False\n",
    ")\n",
    "augment_data(\n",
    "    test_x,\n",
    "    test_y,\n",
    "    base_path / \"data\",\n",
    "    base_path / \"new_data/test\",\n",
    "    augment=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import seeding, create_dir, sort_path_list\n",
    "from Hyperparams import Hyperparams\n",
    "from data import DriveDataset\n",
    "from unet import build_unet\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/zhome/52/3/174111/auto_tetris')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seeding(42)\n",
    "base_path = Path.cwd().parent\n",
    "base_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size:\n",
      "Train: 140 - Valid: 30\n"
     ]
    }
   ],
   "source": [
    "# Load train dataset\n",
    "train_x = list((base_path / \"new_data/train/images/\").glob(\"*.jpeg\"))\n",
    "train_y = list((base_path / \"new_data/train/masks/\").glob(\"*.jpeg\"))\n",
    "\n",
    "# Sort dataset, so images and masks match\n",
    "train_x.sort(key=sort_path_list)\n",
    "train_y.sort(key=sort_path_list)\n",
    "\n",
    "# Load test dataset\n",
    "val_x = list((base_path / \"new_data/val/images/\").glob(\"*.jpeg\"))\n",
    "val_y = list((base_path / \"new_data/val/masks/\").glob(\"*.jpeg\"))\n",
    "\n",
    "# Sort dataset, so images and masks match\n",
    "val_x.sort(key=sort_path_list)\n",
    "val_y.sort(key=sort_path_list)\n",
    "\n",
    "data_str = f\"Dataset size:\\nTrain: {len(train_x)} - Valid: {len(val_x)}\"\n",
    "print(data_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories\n",
    "create_dir(base_path / \"checkpoints\")\n",
    "checkpoint_path = base_path / \"checkpoints/checkpoint.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "\n",
    "hyperparams = Hyperparams(base_path / \"train_conf.toml\")\n",
    "\n",
    "H = hyperparams.image_size\n",
    "W = hyperparams.image_size\n",
    "size = (H, W)\n",
    "\n",
    "batch_size = hyperparams.batch_size\n",
    "num_epochs = 12#hyperparams.epochs\n",
    "lr = hyperparams.lr\n",
    "layers = hyperparams.layers\n",
    "early_stopping_patience = 5\n",
    "\n",
    "# Dataset and Dataloader\n",
    "train_dataset = DriveDataset(train_x, train_y)\n",
    "val_dataset = DriveDataset(val_x, val_y)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset, batch_size=batch_size, shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = build_unet(layers=layers)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# optimizer = hyperparams.optimizer(model.parameters(), lr=hyperparams.lr)\n",
    "\n",
    "optimizer = hyperparams.optimizer_class(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, verbose=True)\n",
    "loss_fn = hyperparams.loss_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import epoch_time\n",
    "import time\n",
    "from train import train, evaluate\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create DataFrame\n",
    "trained_models_df = pd.DataFrame(columns=[\n",
    "    'filename',\n",
    "    'train_start_datetime',\n",
    "    'batch_size',\n",
    "    'lr',\n",
    "    'layers',\n",
    "    'optimizer',\n",
    "    'loss',\n",
    "    'trained_epochs',\n",
    "    'last_saved_epoch',\n",
    "    'best_val_loss',\n",
    "    'early_stopping',\n",
    "    'epoch_train_losses',\n",
    "    'epoch_val_losses',\n",
    "    'epoch_train_times',\n",
    "    'total_train_time',\n",
    "])\n",
    "\n",
    "create_dir(base_path / \"models\")\n",
    "TRAINED_MODELS_CSV = base_path / \"models/trained_models.csv\"\n",
    "\n",
    "isExist = os.path.exists(TRAINED_MODELS_CSV)\n",
    "if not isExist:\n",
    "    trained_models_df.to_csv(TRAINED_MODELS_CSV)\n",
    "    print(\"Created trained_models.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022-12-04 20:25:52.578568'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_start_datetime = datetime.now()\n",
    "str(train_start_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss improved from 1.3540 to 1.3502. Saving checkpoint: /zhome/52/3/174111/auto_tetris/checkpoints/checkpoint.pth\n",
      "Epoch: 10 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 1.348\n",
      "\t Val. Loss: 1.350\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss improved from 1.3502 to 1.3465. Saving checkpoint: /zhome/52/3/174111/auto_tetris/checkpoints/checkpoint.pth\n",
      "Epoch: 11 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 1.344\n",
      "\t Val. Loss: 1.346\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid loss improved from 1.3465 to 1.3428. Saving checkpoint: /zhome/52/3/174111/auto_tetris/checkpoints/checkpoint.pth\n",
      "Epoch: 12 | Epoch Time: 0m 2s\n",
      "\tTrain Loss: 1.339\n",
      "\t Val. Loss: 1.343\n",
      "\n",
      "Training completed:\n",
      "\tEpochs: \t12\n",
      "\tTraining Time: \t0m 30s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "\n",
    "train_start_datetime = datetime.now()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "epoch_times = []\n",
    "\n",
    "trained_epochs = 0\n",
    "last_saved_epoch = 0\n",
    "early_stopping = False\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "train_start = time.time()\n",
    "for epoch in range(num_epochs):\n",
    "    start_time = time.time()\n",
    "\n",
    "    train_loss = train(model, train_loader, optimizer, loss_fn, device)\n",
    "    val_loss = evaluate(model, val_loader, loss_fn, device)\n",
    "\n",
    "    \"\"\" Saving the model \"\"\"\n",
    "    if val_loss < best_val_loss:\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "    if epoch % 5 == 4:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "    if val_loss < best_val_loss:\n",
    "        data_str = f\"Valid loss improved from {best_val_loss:2.4f} to {val_loss:2.4f}. Saving checkpoint: {checkpoint_path}\"\n",
    "        print(data_str)\n",
    "        \n",
    "        best_val_loss = val_loss\n",
    "        last_saved_epoch = epoch+1\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    trained_epochs = epoch+1\n",
    "    data_str = f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\\n'\n",
    "    data_str += f'\\tTrain Loss: {train_loss:.3f}\\n'\n",
    "    data_str += f'\\t Val. Loss: {val_loss:.3f}\\n'\n",
    "    print(data_str)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    epoch_times.append(end_time - start_time)\n",
    "\n",
    "\n",
    "    if trained_epochs - last_saved_epoch >= early_stopping_patience:\n",
    "        early_stopping = True\n",
    "        break\n",
    "\n",
    "\n",
    "train_end = time.time()\n",
    "train_mins, train_secs = epoch_time(train_start, train_end)\n",
    "data_str = f'Training completed:\\n'\n",
    "data_str += f'\\tEpochs: \\t{trained_epochs}\\n'\n",
    "data_str += f'\\tTraining Time: \\t{train_mins}m {train_secs}s\\n'\n",
    "# data_str += f'\\tTrain Loss: \\t{train_loss:.3f}\\n'\n",
    "# data_str += f'\\t Val. Loss: \\t{val_loss:.3f}\\n'\n",
    "print(data_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_to_append = {\n",
    "    'filename': [hyperparams.model_name()],\n",
    "    'train_start_datetime': [str(train_start_datetime)],\n",
    "    'batch_size': [hyperparams.batch_size],\n",
    "    'lr': [hyperparams.lr],\n",
    "    'layers': [hyperparams.layers],\n",
    "    'optimizer': [hyperparams.optimizer],\n",
    "    'loss': [hyperparams.loss],\n",
    "    'trained_epochs': [trained_epochs],\n",
    "    'last_saved_epoch': [last_saved_epoch],\n",
    "    'best_val_loss': [best_val_loss],\n",
    "    'early_stopping': [early_stopping],\n",
    "    'epoch_train_losses': [json.dumps(train_losses)],\n",
    "    'epoch_val_losses': [json.dumps(val_losses)],\n",
    "    'epoch_train_times': [json.dumps(epoch_times)],\n",
    "    'total_train_time': [train_end - train_start],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_row = pd.DataFrame.from_dict(dict_to_append)\n",
    "# trained_models_df = pd.concat([trained_models_df, new_row], ignore_index=True)\n",
    "new_row.to_csv(TRAINED_MODELS_CSV, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: filename already exists, couldn't save /zhome/52/3/174111/auto_tetris/models/roof_surface_model_B20_E100_lr1.000e-04_L4_Adam_DiceBCE.pth\n"
     ]
    }
   ],
   "source": [
    "model_save_path = base_path / \"models\" / hyperparams.model_name()\n",
    "\n",
    "isExist = os.path.exists(model_save_path)\n",
    "if not isExist:\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    print(f\"Saved model to {model_save_path}\")\n",
    "else:\n",
    "    print(f\"WARNING: filename already exists, couldn't save {model_save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
